{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sense of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X, Y = np.loadtxt('pizza.txt', skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Reservations\", fontsize=20)  # Print the X label\n",
    "plt.ylabel(\"Pizzas\", fontsize=20)        # Print the Y label\n",
    "plt.axis([0, 50, 0, 50])                 # Both axes range from 0 to 50\n",
    "plt.plot(X, Y, \"bo\")                     # Plot the data as blue circles (that's what \"bo\" stands for)\n",
    "plt.show()                               # Visualize the diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcasting, X is an array\n",
    "def predict(X, w, b):\n",
    "    return X * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27.6, 14.4, 28.8, 39.6, 27.6, 27.6, 13.2, 33.6, 20.4, 24. , 43.2,\n",
       "       15.6, 15.6, 37.2, 38.4, 14.4, 44.4, 19.2, 24. , 33.6, 30. , 22.8,\n",
       "       43.2, 21.6, 30. , 24. , 37.2, 18. , 19.2, 27.6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w, b):\n",
    "    predictions = predict(X, w, b)\n",
    "    return np.average((predictions - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.778666666666677"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X, Y, 1.2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X, Y, w, b)\n",
    "        print(\"Iteration %4d => Loss: %.6f\" % (i, current_loss))\n",
    "\n",
    "        if loss(X, Y, w - lr, b) < current_loss:\n",
    "            w -= lr\n",
    "        elif loss(X, Y, w + lr, b) < current_loss:\n",
    "            w += lr\n",
    "        elif loss(X, Y, w, b - lr) < current_loss:\n",
    "            b -= lr\n",
    "        elif loss(X, Y, w, b + lr) < current_loss:\n",
    "            b += lr\n",
    "        else:\n",
    "            return w, b\n",
    "\n",
    "    raise Exception(\"Couldn't find a result within %d iterations\" % iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 812.866667\n",
      "Iteration    1 => Loss: 804.820547\n",
      "Iteration    2 => Loss: 796.818187\n",
      "Iteration    3 => Loss: 788.859587\n",
      "Iteration    4 => Loss: 780.944747\n",
      "Iteration    5 => Loss: 773.073667\n",
      "Iteration    6 => Loss: 765.246347\n",
      "Iteration    7 => Loss: 757.462787\n",
      "Iteration    8 => Loss: 749.722987\n",
      "Iteration    9 => Loss: 742.026947\n",
      "Iteration   10 => Loss: 734.374667\n",
      "Iteration   11 => Loss: 726.766147\n",
      "Iteration   12 => Loss: 719.201387\n",
      "Iteration   13 => Loss: 711.680387\n",
      "Iteration   14 => Loss: 704.203147\n",
      "Iteration   15 => Loss: 696.769667\n",
      "Iteration   16 => Loss: 689.379947\n",
      "Iteration   17 => Loss: 682.033987\n",
      "Iteration   18 => Loss: 674.731787\n",
      "Iteration   19 => Loss: 667.473347\n",
      "Iteration   20 => Loss: 660.258667\n",
      "Iteration   21 => Loss: 653.087747\n",
      "Iteration   22 => Loss: 645.960587\n",
      "Iteration   23 => Loss: 638.877187\n",
      "Iteration   24 => Loss: 631.837547\n",
      "Iteration   25 => Loss: 624.841667\n",
      "Iteration   26 => Loss: 617.889547\n",
      "Iteration   27 => Loss: 610.981187\n",
      "Iteration   28 => Loss: 604.116587\n",
      "Iteration   29 => Loss: 597.295747\n",
      "Iteration   30 => Loss: 590.518667\n",
      "Iteration   31 => Loss: 583.785347\n",
      "Iteration   32 => Loss: 577.095787\n",
      "Iteration   33 => Loss: 570.449987\n",
      "Iteration   34 => Loss: 563.847947\n",
      "Iteration   35 => Loss: 557.289667\n",
      "Iteration   36 => Loss: 550.775147\n",
      "Iteration   37 => Loss: 544.304387\n",
      "Iteration   38 => Loss: 537.877387\n",
      "Iteration   39 => Loss: 531.494147\n",
      "Iteration   40 => Loss: 525.154667\n",
      "Iteration   41 => Loss: 518.858947\n",
      "Iteration   42 => Loss: 512.606987\n",
      "Iteration   43 => Loss: 506.398787\n",
      "Iteration   44 => Loss: 500.234347\n",
      "Iteration   45 => Loss: 494.113667\n",
      "Iteration   46 => Loss: 488.036747\n",
      "Iteration   47 => Loss: 482.003587\n",
      "Iteration   48 => Loss: 476.014187\n",
      "Iteration   49 => Loss: 470.068547\n",
      "Iteration   50 => Loss: 464.166667\n",
      "Iteration   51 => Loss: 458.308547\n",
      "Iteration   52 => Loss: 452.494187\n",
      "Iteration   53 => Loss: 446.723587\n",
      "Iteration   54 => Loss: 440.996747\n",
      "Iteration   55 => Loss: 435.313667\n",
      "Iteration   56 => Loss: 429.674347\n",
      "Iteration   57 => Loss: 424.078787\n",
      "Iteration   58 => Loss: 418.526987\n",
      "Iteration   59 => Loss: 413.018947\n",
      "Iteration   60 => Loss: 407.554667\n",
      "Iteration   61 => Loss: 402.134147\n",
      "Iteration   62 => Loss: 396.757387\n",
      "Iteration   63 => Loss: 391.424387\n",
      "Iteration   64 => Loss: 386.135147\n",
      "Iteration   65 => Loss: 380.889667\n",
      "Iteration   66 => Loss: 375.687947\n",
      "Iteration   67 => Loss: 370.529987\n",
      "Iteration   68 => Loss: 365.415787\n",
      "Iteration   69 => Loss: 360.345347\n",
      "Iteration   70 => Loss: 355.318667\n",
      "Iteration   71 => Loss: 350.335747\n",
      "Iteration   72 => Loss: 345.396587\n",
      "Iteration   73 => Loss: 340.501187\n",
      "Iteration   74 => Loss: 335.649547\n",
      "Iteration   75 => Loss: 330.841667\n",
      "Iteration   76 => Loss: 326.077547\n",
      "Iteration   77 => Loss: 321.357187\n",
      "Iteration   78 => Loss: 316.680587\n",
      "Iteration   79 => Loss: 312.047747\n",
      "Iteration   80 => Loss: 307.458667\n",
      "Iteration   81 => Loss: 302.913347\n",
      "Iteration   82 => Loss: 298.411787\n",
      "Iteration   83 => Loss: 293.953987\n",
      "Iteration   84 => Loss: 289.539947\n",
      "Iteration   85 => Loss: 285.169667\n",
      "Iteration   86 => Loss: 280.843147\n",
      "Iteration   87 => Loss: 276.560387\n",
      "Iteration   88 => Loss: 272.321387\n",
      "Iteration   89 => Loss: 268.126147\n",
      "Iteration   90 => Loss: 263.974667\n",
      "Iteration   91 => Loss: 259.866947\n",
      "Iteration   92 => Loss: 255.802987\n",
      "Iteration   93 => Loss: 251.782787\n",
      "Iteration   94 => Loss: 247.806347\n",
      "Iteration   95 => Loss: 243.873667\n",
      "Iteration   96 => Loss: 239.984747\n",
      "Iteration   97 => Loss: 236.139587\n",
      "Iteration   98 => Loss: 232.338187\n",
      "Iteration   99 => Loss: 228.580547\n",
      "Iteration  100 => Loss: 224.866667\n",
      "Iteration  101 => Loss: 221.196547\n",
      "Iteration  102 => Loss: 217.570187\n",
      "Iteration  103 => Loss: 213.987587\n",
      "Iteration  104 => Loss: 210.448747\n",
      "Iteration  105 => Loss: 206.953667\n",
      "Iteration  106 => Loss: 203.502347\n",
      "Iteration  107 => Loss: 200.094787\n",
      "Iteration  108 => Loss: 196.730987\n",
      "Iteration  109 => Loss: 193.410947\n",
      "Iteration  110 => Loss: 190.134667\n",
      "Iteration  111 => Loss: 186.902147\n",
      "Iteration  112 => Loss: 183.713387\n",
      "Iteration  113 => Loss: 180.568387\n",
      "Iteration  114 => Loss: 177.467147\n",
      "Iteration  115 => Loss: 174.409667\n",
      "Iteration  116 => Loss: 171.395947\n",
      "Iteration  117 => Loss: 168.425987\n",
      "Iteration  118 => Loss: 165.499787\n",
      "Iteration  119 => Loss: 162.617347\n",
      "Iteration  120 => Loss: 159.778667\n",
      "Iteration  121 => Loss: 156.983747\n",
      "Iteration  122 => Loss: 154.232587\n",
      "Iteration  123 => Loss: 151.525187\n",
      "Iteration  124 => Loss: 148.861547\n",
      "Iteration  125 => Loss: 146.241667\n",
      "Iteration  126 => Loss: 143.665547\n",
      "Iteration  127 => Loss: 141.133187\n",
      "Iteration  128 => Loss: 138.644587\n",
      "Iteration  129 => Loss: 136.199747\n",
      "Iteration  130 => Loss: 133.798667\n",
      "Iteration  131 => Loss: 131.441347\n",
      "Iteration  132 => Loss: 129.127787\n",
      "Iteration  133 => Loss: 126.857987\n",
      "Iteration  134 => Loss: 124.631947\n",
      "Iteration  135 => Loss: 122.449667\n",
      "Iteration  136 => Loss: 120.311147\n",
      "Iteration  137 => Loss: 118.216387\n",
      "Iteration  138 => Loss: 116.165387\n",
      "Iteration  139 => Loss: 114.158147\n",
      "Iteration  140 => Loss: 112.194667\n",
      "Iteration  141 => Loss: 110.274947\n",
      "Iteration  142 => Loss: 108.398987\n",
      "Iteration  143 => Loss: 106.566787\n",
      "Iteration  144 => Loss: 104.778347\n",
      "Iteration  145 => Loss: 103.033667\n",
      "Iteration  146 => Loss: 101.332747\n",
      "Iteration  147 => Loss: 99.675587\n",
      "Iteration  148 => Loss: 98.062187\n",
      "Iteration  149 => Loss: 96.492547\n",
      "Iteration  150 => Loss: 94.966667\n",
      "Iteration  151 => Loss: 93.484547\n",
      "Iteration  152 => Loss: 92.046187\n",
      "Iteration  153 => Loss: 90.651587\n",
      "Iteration  154 => Loss: 89.300747\n",
      "Iteration  155 => Loss: 87.993667\n",
      "Iteration  156 => Loss: 86.730347\n",
      "Iteration  157 => Loss: 85.510787\n",
      "Iteration  158 => Loss: 84.334987\n",
      "Iteration  159 => Loss: 83.202947\n",
      "Iteration  160 => Loss: 82.114667\n",
      "Iteration  161 => Loss: 81.070147\n",
      "Iteration  162 => Loss: 80.069387\n",
      "Iteration  163 => Loss: 79.112387\n",
      "Iteration  164 => Loss: 78.199147\n",
      "Iteration  165 => Loss: 77.329667\n",
      "Iteration  166 => Loss: 76.503947\n",
      "Iteration  167 => Loss: 75.721987\n",
      "Iteration  168 => Loss: 74.983787\n",
      "Iteration  169 => Loss: 74.289347\n",
      "Iteration  170 => Loss: 73.638667\n",
      "Iteration  171 => Loss: 73.031747\n",
      "Iteration  172 => Loss: 72.468587\n",
      "Iteration  173 => Loss: 71.949187\n",
      "Iteration  174 => Loss: 71.473547\n",
      "Iteration  175 => Loss: 71.041667\n",
      "Iteration  176 => Loss: 70.653547\n",
      "Iteration  177 => Loss: 70.309187\n",
      "Iteration  178 => Loss: 70.008587\n",
      "Iteration  179 => Loss: 69.751747\n",
      "Iteration  180 => Loss: 69.538667\n",
      "Iteration  181 => Loss: 69.369347\n",
      "Iteration  182 => Loss: 69.243787\n",
      "Iteration  183 => Loss: 69.161987\n",
      "Iteration  184 => Loss: 69.123947\n",
      "Iteration  185 => Loss: 69.052847\n",
      "Iteration  186 => Loss: 68.981947\n",
      "Iteration  187 => Loss: 68.911247\n",
      "Iteration  188 => Loss: 68.840747\n",
      "Iteration  189 => Loss: 68.770447\n",
      "Iteration  190 => Loss: 68.700347\n",
      "Iteration  191 => Loss: 68.630447\n",
      "Iteration  192 => Loss: 68.560747\n",
      "Iteration  193 => Loss: 68.491247\n",
      "Iteration  194 => Loss: 68.421947\n",
      "Iteration  195 => Loss: 68.352847\n",
      "Iteration  196 => Loss: 68.283947\n",
      "Iteration  197 => Loss: 68.215247\n",
      "Iteration  198 => Loss: 68.146747\n",
      "Iteration  199 => Loss: 68.078447\n",
      "Iteration  200 => Loss: 68.010347\n",
      "Iteration  201 => Loss: 68.007853\n",
      "Iteration  202 => Loss: 67.937420\n",
      "Iteration  203 => Loss: 67.867187\n",
      "Iteration  204 => Loss: 67.797153\n",
      "Iteration  205 => Loss: 67.727320\n",
      "Iteration  206 => Loss: 67.657687\n",
      "Iteration  207 => Loss: 67.588253\n",
      "Iteration  208 => Loss: 67.519020\n",
      "Iteration  209 => Loss: 67.449987\n",
      "Iteration  210 => Loss: 67.381153\n",
      "Iteration  211 => Loss: 67.312520\n",
      "Iteration  212 => Loss: 67.244087\n",
      "Iteration  213 => Loss: 67.175853\n",
      "Iteration  214 => Loss: 67.107820\n",
      "Iteration  215 => Loss: 67.039987\n",
      "Iteration  216 => Loss: 66.972353\n",
      "Iteration  217 => Loss: 66.904920\n",
      "Iteration  218 => Loss: 66.837687\n",
      "Iteration  219 => Loss: 66.835887\n",
      "Iteration  220 => Loss: 66.766320\n",
      "Iteration  221 => Loss: 66.696953\n",
      "Iteration  222 => Loss: 66.627787\n",
      "Iteration  223 => Loss: 66.558820\n",
      "Iteration  224 => Loss: 66.490053\n",
      "Iteration  225 => Loss: 66.421487\n",
      "Iteration  226 => Loss: 66.353120\n",
      "Iteration  227 => Loss: 66.284953\n",
      "Iteration  228 => Loss: 66.216987\n",
      "Iteration  229 => Loss: 66.149220\n",
      "Iteration  230 => Loss: 66.081653\n",
      "Iteration  231 => Loss: 66.014287\n",
      "Iteration  232 => Loss: 65.947120\n",
      "Iteration  233 => Loss: 65.880153\n",
      "Iteration  234 => Loss: 65.813387\n",
      "Iteration  235 => Loss: 65.746820\n",
      "Iteration  236 => Loss: 65.680453\n",
      "Iteration  237 => Loss: 65.679347\n",
      "Iteration  238 => Loss: 65.610647\n",
      "Iteration  239 => Loss: 65.542147\n",
      "Iteration  240 => Loss: 65.473847\n",
      "Iteration  241 => Loss: 65.405747\n",
      "Iteration  242 => Loss: 65.337847\n",
      "Iteration  243 => Loss: 65.270147\n",
      "Iteration  244 => Loss: 65.202647\n",
      "Iteration  245 => Loss: 65.135347\n",
      "Iteration  246 => Loss: 65.068247\n",
      "Iteration  247 => Loss: 65.001347\n",
      "Iteration  248 => Loss: 64.934647\n",
      "Iteration  249 => Loss: 64.868147\n",
      "Iteration  250 => Loss: 64.801847\n",
      "Iteration  251 => Loss: 64.735747\n",
      "Iteration  252 => Loss: 64.669847\n",
      "Iteration  253 => Loss: 64.604147\n",
      "Iteration  254 => Loss: 64.538647\n",
      "Iteration  255 => Loss: 64.538233\n",
      "Iteration  256 => Loss: 64.470400\n",
      "Iteration  257 => Loss: 64.402767\n",
      "Iteration  258 => Loss: 64.335333\n",
      "Iteration  259 => Loss: 64.268100\n",
      "Iteration  260 => Loss: 64.201067\n",
      "Iteration  261 => Loss: 64.134233\n",
      "Iteration  262 => Loss: 64.067600\n",
      "Iteration  263 => Loss: 64.001167\n",
      "Iteration  264 => Loss: 63.934933\n",
      "Iteration  265 => Loss: 63.868900\n",
      "Iteration  266 => Loss: 63.803067\n",
      "Iteration  267 => Loss: 63.737433\n",
      "Iteration  268 => Loss: 63.672000\n",
      "Iteration  269 => Loss: 63.606767\n",
      "Iteration  270 => Loss: 63.541733\n",
      "Iteration  271 => Loss: 63.476900\n",
      "Iteration  272 => Loss: 63.412267\n",
      "Iteration  273 => Loss: 63.347833\n",
      "Iteration  274 => Loss: 63.345580\n",
      "Iteration  275 => Loss: 63.278813\n",
      "Iteration  276 => Loss: 63.212247\n",
      "Iteration  277 => Loss: 63.145880\n",
      "Iteration  278 => Loss: 63.079713\n",
      "Iteration  279 => Loss: 63.013747\n",
      "Iteration  280 => Loss: 62.947980\n",
      "Iteration  281 => Loss: 62.882413\n",
      "Iteration  282 => Loss: 62.817047\n",
      "Iteration  283 => Loss: 62.751880\n",
      "Iteration  284 => Loss: 62.686913\n",
      "Iteration  285 => Loss: 62.622147\n",
      "Iteration  286 => Loss: 62.557580\n",
      "Iteration  287 => Loss: 62.493213\n",
      "Iteration  288 => Loss: 62.429047\n",
      "Iteration  289 => Loss: 62.365080\n",
      "Iteration  290 => Loss: 62.301313\n",
      "Iteration  291 => Loss: 62.237747\n",
      "Iteration  292 => Loss: 62.236187\n",
      "Iteration  293 => Loss: 62.170287\n",
      "Iteration  294 => Loss: 62.104587\n",
      "Iteration  295 => Loss: 62.039087\n",
      "Iteration  296 => Loss: 61.973787\n",
      "Iteration  297 => Loss: 61.908687\n",
      "Iteration  298 => Loss: 61.843787\n",
      "Iteration  299 => Loss: 61.779087\n",
      "Iteration  300 => Loss: 61.714587\n",
      "Iteration  301 => Loss: 61.650287\n",
      "Iteration  302 => Loss: 61.586187\n",
      "Iteration  303 => Loss: 61.522287\n",
      "Iteration  304 => Loss: 61.458587\n",
      "Iteration  305 => Loss: 61.395087\n",
      "Iteration  306 => Loss: 61.331787\n",
      "Iteration  307 => Loss: 61.268687\n",
      "Iteration  308 => Loss: 61.205787\n",
      "Iteration  309 => Loss: 61.143087\n",
      "Iteration  310 => Loss: 61.142220\n",
      "Iteration  311 => Loss: 61.077187\n",
      "Iteration  312 => Loss: 61.012353\n",
      "Iteration  313 => Loss: 60.947720\n",
      "Iteration  314 => Loss: 60.883287\n",
      "Iteration  315 => Loss: 60.819053\n",
      "Iteration  316 => Loss: 60.755020\n",
      "Iteration  317 => Loss: 60.691187\n",
      "Iteration  318 => Loss: 60.627553\n",
      "Iteration  319 => Loss: 60.564120\n",
      "Iteration  320 => Loss: 60.500887\n",
      "Iteration  321 => Loss: 60.437853\n",
      "Iteration  322 => Loss: 60.375020\n",
      "Iteration  323 => Loss: 60.312387\n",
      "Iteration  324 => Loss: 60.249953\n",
      "Iteration  325 => Loss: 60.187720\n",
      "Iteration  326 => Loss: 60.125687\n",
      "Iteration  327 => Loss: 60.063853\n",
      "Iteration  328 => Loss: 60.063680\n",
      "Iteration  329 => Loss: 59.999513\n",
      "Iteration  330 => Loss: 59.935547\n",
      "Iteration  331 => Loss: 59.871780\n",
      "Iteration  332 => Loss: 59.808213\n",
      "Iteration  333 => Loss: 59.744847\n",
      "Iteration  334 => Loss: 59.681680\n",
      "Iteration  335 => Loss: 59.618713\n",
      "Iteration  336 => Loss: 59.555947\n",
      "Iteration  337 => Loss: 59.493380\n",
      "Iteration  338 => Loss: 59.431013\n",
      "Iteration  339 => Loss: 59.368847\n",
      "Iteration  340 => Loss: 59.306880\n",
      "Iteration  341 => Loss: 59.245113\n",
      "Iteration  342 => Loss: 59.183547\n",
      "Iteration  343 => Loss: 59.122180\n",
      "Iteration  344 => Loss: 59.061013\n",
      "Iteration  345 => Loss: 59.000047\n",
      "Iteration  346 => Loss: 58.939280\n",
      "Iteration  347 => Loss: 58.937267\n",
      "Iteration  348 => Loss: 58.874167\n",
      "Iteration  349 => Loss: 58.811267\n",
      "Iteration  350 => Loss: 58.748567\n",
      "Iteration  351 => Loss: 58.686067\n",
      "Iteration  352 => Loss: 58.623767\n",
      "Iteration  353 => Loss: 58.561667\n",
      "Iteration  354 => Loss: 58.499767\n",
      "Iteration  355 => Loss: 58.438067\n",
      "Iteration  356 => Loss: 58.376567\n",
      "Iteration  357 => Loss: 58.315267\n",
      "Iteration  358 => Loss: 58.254167\n",
      "Iteration  359 => Loss: 58.193267\n",
      "Iteration  360 => Loss: 58.132567\n",
      "Iteration  361 => Loss: 58.072067\n",
      "Iteration  362 => Loss: 58.011767\n",
      "Iteration  363 => Loss: 57.951667\n",
      "Iteration  364 => Loss: 57.891767\n",
      "Iteration  365 => Loss: 57.890447\n",
      "Iteration  366 => Loss: 57.828213\n",
      "Iteration  367 => Loss: 57.766180\n",
      "Iteration  368 => Loss: 57.704347\n",
      "Iteration  369 => Loss: 57.642713\n",
      "Iteration  370 => Loss: 57.581280\n",
      "Iteration  371 => Loss: 57.520047\n",
      "Iteration  372 => Loss: 57.459013\n",
      "Iteration  373 => Loss: 57.398180\n",
      "Iteration  374 => Loss: 57.337547\n",
      "Iteration  375 => Loss: 57.277113\n",
      "Iteration  376 => Loss: 57.216880\n",
      "Iteration  377 => Loss: 57.156847\n",
      "Iteration  378 => Loss: 57.097013\n",
      "Iteration  379 => Loss: 57.037380\n",
      "Iteration  380 => Loss: 56.977947\n",
      "Iteration  381 => Loss: 56.918713\n",
      "Iteration  382 => Loss: 56.859680\n",
      "Iteration  383 => Loss: 56.859053\n",
      "Iteration  384 => Loss: 56.797687\n",
      "Iteration  385 => Loss: 56.736520\n",
      "Iteration  386 => Loss: 56.675553\n",
      "Iteration  387 => Loss: 56.614787\n",
      "Iteration  388 => Loss: 56.554220\n",
      "Iteration  389 => Loss: 56.493853\n",
      "Iteration  390 => Loss: 56.433687\n",
      "Iteration  391 => Loss: 56.373720\n",
      "Iteration  392 => Loss: 56.313953\n",
      "Iteration  393 => Loss: 56.254387\n",
      "Iteration  394 => Loss: 56.195020\n",
      "Iteration  395 => Loss: 56.135853\n",
      "Iteration  396 => Loss: 56.076887\n",
      "Iteration  397 => Loss: 56.018120\n",
      "Iteration  398 => Loss: 55.959553\n",
      "Iteration  399 => Loss: 55.901187\n",
      "Iteration  400 => Loss: 55.843020\n",
      "Iteration  401 => Loss: 55.785053\n",
      "Iteration  402 => Loss: 55.782587\n",
      "Iteration  403 => Loss: 55.722287\n",
      "Iteration  404 => Loss: 55.662187\n",
      "Iteration  405 => Loss: 55.602287\n",
      "Iteration  406 => Loss: 55.542587\n",
      "Iteration  407 => Loss: 55.483087\n",
      "Iteration  408 => Loss: 55.423787\n",
      "Iteration  409 => Loss: 55.364687\n",
      "Iteration  410 => Loss: 55.305787\n",
      "Iteration  411 => Loss: 55.247087\n",
      "Iteration  412 => Loss: 55.188587\n",
      "Iteration  413 => Loss: 55.130287\n",
      "Iteration  414 => Loss: 55.072187\n",
      "Iteration  415 => Loss: 55.014287\n",
      "Iteration  416 => Loss: 54.956587\n",
      "Iteration  417 => Loss: 54.899087\n",
      "Iteration  418 => Loss: 54.841787\n",
      "Iteration  419 => Loss: 54.784687\n",
      "Iteration  420 => Loss: 54.782913\n",
      "Iteration  421 => Loss: 54.723480\n",
      "Iteration  422 => Loss: 54.664247\n",
      "Iteration  423 => Loss: 54.605213\n",
      "Iteration  424 => Loss: 54.546380\n",
      "Iteration  425 => Loss: 54.487747\n",
      "Iteration  426 => Loss: 54.429313\n",
      "Iteration  427 => Loss: 54.371080\n",
      "Iteration  428 => Loss: 54.313047\n",
      "Iteration  429 => Loss: 54.255213\n",
      "Iteration  430 => Loss: 54.197580\n",
      "Iteration  431 => Loss: 54.140147\n",
      "Iteration  432 => Loss: 54.082913\n",
      "Iteration  433 => Loss: 54.025880\n",
      "Iteration  434 => Loss: 53.969047\n",
      "Iteration  435 => Loss: 53.912413\n",
      "Iteration  436 => Loss: 53.855980\n",
      "Iteration  437 => Loss: 53.799747\n",
      "Iteration  438 => Loss: 53.798667\n",
      "Iteration  439 => Loss: 53.740100\n",
      "Iteration  440 => Loss: 53.681733\n",
      "Iteration  441 => Loss: 53.623567\n",
      "Iteration  442 => Loss: 53.565600\n",
      "Iteration  443 => Loss: 53.507833\n",
      "Iteration  444 => Loss: 53.450267\n",
      "Iteration  445 => Loss: 53.392900\n",
      "Iteration  446 => Loss: 53.335733\n",
      "Iteration  447 => Loss: 53.278767\n",
      "Iteration  448 => Loss: 53.222000\n",
      "Iteration  449 => Loss: 53.165433\n",
      "Iteration  450 => Loss: 53.109067\n",
      "Iteration  451 => Loss: 53.052900\n",
      "Iteration  452 => Loss: 52.996933\n",
      "Iteration  453 => Loss: 52.941167\n",
      "Iteration  454 => Loss: 52.885600\n",
      "Iteration  455 => Loss: 52.830233\n",
      "Iteration  456 => Loss: 52.829847\n",
      "Iteration  457 => Loss: 52.772147\n",
      "Iteration  458 => Loss: 52.714647\n",
      "Iteration  459 => Loss: 52.657347\n",
      "Iteration  460 => Loss: 52.600247\n",
      "Iteration  461 => Loss: 52.543347\n",
      "Iteration  462 => Loss: 52.486647\n",
      "Iteration  463 => Loss: 52.430147\n",
      "Iteration  464 => Loss: 52.373847\n",
      "Iteration  465 => Loss: 52.317747\n",
      "Iteration  466 => Loss: 52.261847\n",
      "Iteration  467 => Loss: 52.206147\n",
      "Iteration  468 => Loss: 52.150647\n",
      "Iteration  469 => Loss: 52.095347\n",
      "Iteration  470 => Loss: 52.040247\n",
      "Iteration  471 => Loss: 51.985347\n",
      "Iteration  472 => Loss: 51.930647\n",
      "Iteration  473 => Loss: 51.876147\n",
      "Iteration  474 => Loss: 51.821847\n",
      "Iteration  475 => Loss: 51.819620\n",
      "Iteration  476 => Loss: 51.762987\n",
      "Iteration  477 => Loss: 51.706553\n",
      "Iteration  478 => Loss: 51.650320\n",
      "Iteration  479 => Loss: 51.594287\n",
      "Iteration  480 => Loss: 51.538453\n",
      "Iteration  481 => Loss: 51.482820\n",
      "Iteration  482 => Loss: 51.427387\n",
      "Iteration  483 => Loss: 51.372153\n",
      "Iteration  484 => Loss: 51.317120\n",
      "Iteration  485 => Loss: 51.262287\n",
      "Iteration  486 => Loss: 51.207653\n",
      "Iteration  487 => Loss: 51.153220\n",
      "Iteration  488 => Loss: 51.098987\n",
      "Iteration  489 => Loss: 51.044953\n",
      "Iteration  490 => Loss: 50.991120\n",
      "Iteration  491 => Loss: 50.937487\n",
      "Iteration  492 => Loss: 50.884053\n",
      "Iteration  493 => Loss: 50.882520\n",
      "Iteration  494 => Loss: 50.826753\n",
      "Iteration  495 => Loss: 50.771187\n",
      "Iteration  496 => Loss: 50.715820\n",
      "Iteration  497 => Loss: 50.660653\n",
      "Iteration  498 => Loss: 50.605687\n",
      "Iteration  499 => Loss: 50.550920\n",
      "Iteration  500 => Loss: 50.496353\n",
      "Iteration  501 => Loss: 50.441987\n",
      "Iteration  502 => Loss: 50.387820\n",
      "Iteration  503 => Loss: 50.333853\n",
      "Iteration  504 => Loss: 50.280087\n",
      "Iteration  505 => Loss: 50.226520\n",
      "Iteration  506 => Loss: 50.173153\n",
      "Iteration  507 => Loss: 50.119987\n",
      "Iteration  508 => Loss: 50.067020\n",
      "Iteration  509 => Loss: 50.014253\n",
      "Iteration  510 => Loss: 49.961687\n",
      "Iteration  511 => Loss: 49.960847\n",
      "Iteration  512 => Loss: 49.905947\n",
      "Iteration  513 => Loss: 49.851247\n",
      "Iteration  514 => Loss: 49.796747\n",
      "Iteration  515 => Loss: 49.742447\n",
      "Iteration  516 => Loss: 49.688347\n",
      "Iteration  517 => Loss: 49.634447\n",
      "Iteration  518 => Loss: 49.580747\n",
      "Iteration  519 => Loss: 49.527247\n",
      "Iteration  520 => Loss: 49.473947\n",
      "Iteration  521 => Loss: 49.420847\n",
      "Iteration  522 => Loss: 49.367947\n",
      "Iteration  523 => Loss: 49.315247\n",
      "Iteration  524 => Loss: 49.262747\n",
      "Iteration  525 => Loss: 49.210447\n",
      "Iteration  526 => Loss: 49.158347\n",
      "Iteration  527 => Loss: 49.106447\n",
      "Iteration  528 => Loss: 49.054747\n",
      "Iteration  529 => Loss: 49.054600\n",
      "Iteration  530 => Loss: 49.000567\n",
      "Iteration  531 => Loss: 48.946733\n",
      "Iteration  532 => Loss: 48.893100\n",
      "Iteration  533 => Loss: 48.839667\n",
      "Iteration  534 => Loss: 48.786433\n",
      "Iteration  535 => Loss: 48.733400\n",
      "Iteration  536 => Loss: 48.680567\n",
      "Iteration  537 => Loss: 48.627933\n",
      "Iteration  538 => Loss: 48.575500\n",
      "Iteration  539 => Loss: 48.523267\n",
      "Iteration  540 => Loss: 48.471233\n",
      "Iteration  541 => Loss: 48.419400\n",
      "Iteration  542 => Loss: 48.367767\n",
      "Iteration  543 => Loss: 48.316333\n",
      "Iteration  544 => Loss: 48.265100\n",
      "Iteration  545 => Loss: 48.214067\n",
      "Iteration  546 => Loss: 48.163233\n",
      "Iteration  547 => Loss: 48.112600\n",
      "Iteration  548 => Loss: 48.110613\n",
      "Iteration  549 => Loss: 48.057647\n",
      "Iteration  550 => Loss: 48.004880\n",
      "Iteration  551 => Loss: 47.952313\n",
      "Iteration  552 => Loss: 47.899947\n",
      "Iteration  553 => Loss: 47.847780\n",
      "Iteration  554 => Loss: 47.795813\n",
      "Iteration  555 => Loss: 47.744047\n",
      "Iteration  556 => Loss: 47.692480\n",
      "Iteration  557 => Loss: 47.641113\n",
      "Iteration  558 => Loss: 47.589947\n",
      "Iteration  559 => Loss: 47.538980\n",
      "Iteration  560 => Loss: 47.488213\n",
      "Iteration  561 => Loss: 47.437647\n",
      "Iteration  562 => Loss: 47.387280\n",
      "Iteration  563 => Loss: 47.337113\n",
      "Iteration  564 => Loss: 47.287147\n",
      "Iteration  565 => Loss: 47.237380\n",
      "Iteration  566 => Loss: 47.236087\n",
      "Iteration  567 => Loss: 47.183987\n",
      "Iteration  568 => Loss: 47.132087\n",
      "Iteration  569 => Loss: 47.080387\n",
      "Iteration  570 => Loss: 47.028887\n",
      "Iteration  571 => Loss: 46.977587\n",
      "Iteration  572 => Loss: 46.926487\n",
      "Iteration  573 => Loss: 46.875587\n",
      "Iteration  574 => Loss: 46.824887\n",
      "Iteration  575 => Loss: 46.774387\n",
      "Iteration  576 => Loss: 46.724087\n",
      "Iteration  577 => Loss: 46.673987\n",
      "Iteration  578 => Loss: 46.624087\n",
      "Iteration  579 => Loss: 46.574387\n",
      "Iteration  580 => Loss: 46.524887\n",
      "Iteration  581 => Loss: 46.475587\n",
      "Iteration  582 => Loss: 46.426487\n",
      "Iteration  583 => Loss: 46.377587\n",
      "Iteration  584 => Loss: 46.376987\n",
      "Iteration  585 => Loss: 46.325753\n",
      "Iteration  586 => Loss: 46.274720\n",
      "Iteration  587 => Loss: 46.223887\n",
      "Iteration  588 => Loss: 46.173253\n",
      "Iteration  589 => Loss: 46.122820\n",
      "Iteration  590 => Loss: 46.072587\n",
      "Iteration  591 => Loss: 46.022553\n",
      "Iteration  592 => Loss: 45.972720\n",
      "Iteration  593 => Loss: 45.923087\n",
      "Iteration  594 => Loss: 45.873653\n",
      "Iteration  595 => Loss: 45.824420\n",
      "Iteration  596 => Loss: 45.775387\n",
      "Iteration  597 => Loss: 45.726553\n",
      "Iteration  598 => Loss: 45.677920\n",
      "Iteration  599 => Loss: 45.629487\n",
      "Iteration  600 => Loss: 45.581253\n",
      "Iteration  601 => Loss: 45.533220\n",
      "Iteration  602 => Loss: 45.485387\n",
      "Iteration  603 => Loss: 45.482947\n",
      "Iteration  604 => Loss: 45.432780\n",
      "Iteration  605 => Loss: 45.382813\n",
      "Iteration  606 => Loss: 45.333047\n",
      "Iteration  607 => Loss: 45.283480\n",
      "Iteration  608 => Loss: 45.234113\n",
      "Iteration  609 => Loss: 45.184947\n",
      "Iteration  610 => Loss: 45.135980\n",
      "Iteration  611 => Loss: 45.087213\n",
      "Iteration  612 => Loss: 45.038647\n",
      "Iteration  613 => Loss: 44.990280\n",
      "Iteration  614 => Loss: 44.942113\n",
      "Iteration  615 => Loss: 44.894147\n",
      "Iteration  616 => Loss: 44.846380\n",
      "Iteration  617 => Loss: 44.798813\n",
      "Iteration  618 => Loss: 44.751447\n",
      "Iteration  619 => Loss: 44.704280\n",
      "Iteration  620 => Loss: 44.657313\n",
      "Iteration  621 => Loss: 44.655567\n",
      "Iteration  622 => Loss: 44.606267\n",
      "Iteration  623 => Loss: 44.557167\n",
      "Iteration  624 => Loss: 44.508267\n",
      "Iteration  625 => Loss: 44.459567\n",
      "Iteration  626 => Loss: 44.411067\n",
      "Iteration  627 => Loss: 44.362767\n",
      "Iteration  628 => Loss: 44.314667\n",
      "Iteration  629 => Loss: 44.266767\n",
      "Iteration  630 => Loss: 44.219067\n",
      "Iteration  631 => Loss: 44.171567\n",
      "Iteration  632 => Loss: 44.124267\n",
      "Iteration  633 => Loss: 44.077167\n",
      "Iteration  634 => Loss: 44.030267\n",
      "Iteration  635 => Loss: 43.983567\n",
      "Iteration  636 => Loss: 43.937067\n",
      "Iteration  637 => Loss: 43.890767\n",
      "Iteration  638 => Loss: 43.844667\n",
      "Iteration  639 => Loss: 43.843613\n",
      "Iteration  640 => Loss: 43.795180\n",
      "Iteration  641 => Loss: 43.746947\n",
      "Iteration  642 => Loss: 43.698913\n",
      "Iteration  643 => Loss: 43.651080\n",
      "Iteration  644 => Loss: 43.603447\n",
      "Iteration  645 => Loss: 43.556013\n",
      "Iteration  646 => Loss: 43.508780\n",
      "Iteration  647 => Loss: 43.461747\n",
      "Iteration  648 => Loss: 43.414913\n",
      "Iteration  649 => Loss: 43.368280\n",
      "Iteration  650 => Loss: 43.321847\n",
      "Iteration  651 => Loss: 43.275613\n",
      "Iteration  652 => Loss: 43.229580\n",
      "Iteration  653 => Loss: 43.183747\n",
      "Iteration  654 => Loss: 43.138113\n",
      "Iteration  655 => Loss: 43.092680\n",
      "Iteration  656 => Loss: 43.047447\n",
      "Iteration  657 => Loss: 43.047087\n",
      "Iteration  658 => Loss: 42.999520\n",
      "Iteration  659 => Loss: 42.952153\n",
      "Iteration  660 => Loss: 42.904987\n",
      "Iteration  661 => Loss: 42.858020\n",
      "Iteration  662 => Loss: 42.811253\n",
      "Iteration  663 => Loss: 42.764687\n",
      "Iteration  664 => Loss: 42.718320\n",
      "Iteration  665 => Loss: 42.672153\n",
      "Iteration  666 => Loss: 42.626187\n",
      "Iteration  667 => Loss: 42.580420\n",
      "Iteration  668 => Loss: 42.534853\n",
      "Iteration  669 => Loss: 42.489487\n",
      "Iteration  670 => Loss: 42.444320\n",
      "Iteration  671 => Loss: 42.399353\n",
      "Iteration  672 => Loss: 42.354587\n",
      "Iteration  673 => Loss: 42.310020\n",
      "Iteration  674 => Loss: 42.265653\n",
      "Iteration  675 => Loss: 42.221487\n",
      "Iteration  676 => Loss: 42.219287\n",
      "Iteration  677 => Loss: 42.172787\n",
      "Iteration  678 => Loss: 42.126487\n",
      "Iteration  679 => Loss: 42.080387\n",
      "Iteration  680 => Loss: 42.034487\n",
      "Iteration  681 => Loss: 41.988787\n",
      "Iteration  682 => Loss: 41.943287\n",
      "Iteration  683 => Loss: 41.897987\n",
      "Iteration  684 => Loss: 41.852887\n",
      "Iteration  685 => Loss: 41.807987\n",
      "Iteration  686 => Loss: 41.763287\n",
      "Iteration  687 => Loss: 41.718787\n",
      "Iteration  688 => Loss: 41.674487\n",
      "Iteration  689 => Loss: 41.630387\n",
      "Iteration  690 => Loss: 41.586487\n",
      "Iteration  691 => Loss: 41.542787\n",
      "Iteration  692 => Loss: 41.499287\n",
      "Iteration  693 => Loss: 41.455987\n",
      "Iteration  694 => Loss: 41.454480\n",
      "Iteration  695 => Loss: 41.408847\n",
      "Iteration  696 => Loss: 41.363413\n",
      "Iteration  697 => Loss: 41.318180\n",
      "Iteration  698 => Loss: 41.273147\n",
      "Iteration  699 => Loss: 41.228313\n",
      "Iteration  700 => Loss: 41.183680\n",
      "Iteration  701 => Loss: 41.139247\n",
      "Iteration  702 => Loss: 41.095013\n",
      "Iteration  703 => Loss: 41.050980\n",
      "Iteration  704 => Loss: 41.007147\n",
      "Iteration  705 => Loss: 40.963513\n",
      "Iteration  706 => Loss: 40.920080\n",
      "Iteration  707 => Loss: 40.876847\n",
      "Iteration  708 => Loss: 40.833813\n",
      "Iteration  709 => Loss: 40.790980\n",
      "Iteration  710 => Loss: 40.748347\n",
      "Iteration  711 => Loss: 40.705913\n",
      "Iteration  712 => Loss: 40.705100\n",
      "Iteration  713 => Loss: 40.660333\n",
      "Iteration  714 => Loss: 40.615767\n",
      "Iteration  715 => Loss: 40.571400\n",
      "Iteration  716 => Loss: 40.527233\n",
      "Iteration  717 => Loss: 40.483267\n",
      "Iteration  718 => Loss: 40.439500\n",
      "Iteration  719 => Loss: 40.395933\n",
      "Iteration  720 => Loss: 40.352567\n",
      "Iteration  721 => Loss: 40.309400\n",
      "Iteration  722 => Loss: 40.266433\n",
      "Iteration  723 => Loss: 40.223667\n",
      "Iteration  724 => Loss: 40.181100\n",
      "Iteration  725 => Loss: 40.138733\n",
      "Iteration  726 => Loss: 40.096567\n",
      "Iteration  727 => Loss: 40.054600\n",
      "Iteration  728 => Loss: 40.012833\n",
      "Iteration  729 => Loss: 39.971267\n",
      "Iteration  730 => Loss: 39.971147\n",
      "Iteration  731 => Loss: 39.927247\n",
      "Iteration  732 => Loss: 39.883547\n",
      "Iteration  733 => Loss: 39.840047\n",
      "Iteration  734 => Loss: 39.796747\n",
      "Iteration  735 => Loss: 39.753647\n",
      "Iteration  736 => Loss: 39.710747\n",
      "Iteration  737 => Loss: 39.668047\n",
      "Iteration  738 => Loss: 39.625547\n",
      "Iteration  739 => Loss: 39.583247\n",
      "Iteration  740 => Loss: 39.541147\n",
      "Iteration  741 => Loss: 39.499247\n",
      "Iteration  742 => Loss: 39.457547\n",
      "Iteration  743 => Loss: 39.416047\n",
      "Iteration  744 => Loss: 39.374747\n",
      "Iteration  745 => Loss: 39.333647\n",
      "Iteration  746 => Loss: 39.292747\n",
      "Iteration  747 => Loss: 39.252047\n",
      "Iteration  748 => Loss: 39.211547\n",
      "Iteration  749 => Loss: 39.209587\n",
      "Iteration  750 => Loss: 39.166753\n",
      "Iteration  751 => Loss: 39.124120\n",
      "Iteration  752 => Loss: 39.081687\n",
      "Iteration  753 => Loss: 39.039453\n",
      "Iteration  754 => Loss: 38.997420\n",
      "Iteration  755 => Loss: 38.955587\n",
      "Iteration  756 => Loss: 38.913953\n",
      "Iteration  757 => Loss: 38.872520\n",
      "Iteration  758 => Loss: 38.831287\n",
      "Iteration  759 => Loss: 38.790253\n",
      "Iteration  760 => Loss: 38.749420\n",
      "Iteration  761 => Loss: 38.708787\n",
      "Iteration  762 => Loss: 38.668353\n",
      "Iteration  763 => Loss: 38.628120\n",
      "Iteration  764 => Loss: 38.588087\n",
      "Iteration  765 => Loss: 38.548253\n",
      "Iteration  766 => Loss: 38.508620\n",
      "Iteration  767 => Loss: 38.507353\n",
      "Iteration  768 => Loss: 38.465387\n",
      "Iteration  769 => Loss: 38.423620\n",
      "Iteration  770 => Loss: 38.382053\n",
      "Iteration  771 => Loss: 38.340687\n",
      "Iteration  772 => Loss: 38.299520\n",
      "Iteration  773 => Loss: 38.258553\n",
      "Iteration  774 => Loss: 38.217787\n",
      "Iteration  775 => Loss: 38.177220\n",
      "Iteration  776 => Loss: 38.136853\n",
      "Iteration  777 => Loss: 38.096687\n",
      "Iteration  778 => Loss: 38.056720\n",
      "Iteration  779 => Loss: 38.016953\n",
      "Iteration  780 => Loss: 37.977387\n",
      "Iteration  781 => Loss: 37.938020\n",
      "Iteration  782 => Loss: 37.898853\n",
      "Iteration  783 => Loss: 37.859887\n",
      "Iteration  784 => Loss: 37.821120\n",
      "Iteration  785 => Loss: 37.820547\n",
      "Iteration  786 => Loss: 37.779447\n",
      "Iteration  787 => Loss: 37.738547\n",
      "Iteration  788 => Loss: 37.697847\n",
      "Iteration  789 => Loss: 37.657347\n",
      "Iteration  790 => Loss: 37.617047\n",
      "Iteration  791 => Loss: 37.576947\n",
      "Iteration  792 => Loss: 37.537047\n",
      "Iteration  793 => Loss: 37.497347\n",
      "Iteration  794 => Loss: 37.457847\n",
      "Iteration  795 => Loss: 37.418547\n",
      "Iteration  796 => Loss: 37.379447\n",
      "Iteration  797 => Loss: 37.340547\n",
      "Iteration  798 => Loss: 37.301847\n",
      "Iteration  799 => Loss: 37.263347\n",
      "Iteration  800 => Loss: 37.225047\n",
      "Iteration  801 => Loss: 37.186947\n",
      "Iteration  802 => Loss: 37.149047\n",
      "Iteration  803 => Loss: 37.111347\n",
      "Iteration  804 => Loss: 37.108933\n",
      "Iteration  805 => Loss: 37.068900\n",
      "Iteration  806 => Loss: 37.029067\n",
      "Iteration  807 => Loss: 36.989433\n",
      "Iteration  808 => Loss: 36.950000\n",
      "Iteration  809 => Loss: 36.910767\n",
      "Iteration  810 => Loss: 36.871733\n",
      "Iteration  811 => Loss: 36.832900\n",
      "Iteration  812 => Loss: 36.794267\n",
      "Iteration  813 => Loss: 36.755833\n",
      "Iteration  814 => Loss: 36.717600\n",
      "Iteration  815 => Loss: 36.679567\n",
      "Iteration  816 => Loss: 36.641733\n",
      "Iteration  817 => Loss: 36.604100\n",
      "Iteration  818 => Loss: 36.566667\n",
      "Iteration  819 => Loss: 36.529433\n",
      "Iteration  820 => Loss: 36.492400\n",
      "Iteration  821 => Loss: 36.455567\n",
      "Iteration  822 => Loss: 36.453847\n",
      "Iteration  823 => Loss: 36.414680\n",
      "Iteration  824 => Loss: 36.375713\n",
      "Iteration  825 => Loss: 36.336947\n",
      "Iteration  826 => Loss: 36.298380\n",
      "Iteration  827 => Loss: 36.260013\n",
      "Iteration  828 => Loss: 36.221847\n",
      "Iteration  829 => Loss: 36.183880\n",
      "Iteration  830 => Loss: 36.146113\n",
      "Iteration  831 => Loss: 36.108547\n",
      "Iteration  832 => Loss: 36.071180\n",
      "Iteration  833 => Loss: 36.034013\n",
      "Iteration  834 => Loss: 35.997047\n",
      "Iteration  835 => Loss: 35.960280\n",
      "Iteration  836 => Loss: 35.923713\n",
      "Iteration  837 => Loss: 35.887347\n",
      "Iteration  838 => Loss: 35.851180\n",
      "Iteration  839 => Loss: 35.815213\n",
      "Iteration  840 => Loss: 35.814187\n",
      "Iteration  841 => Loss: 35.775887\n",
      "Iteration  842 => Loss: 35.737787\n",
      "Iteration  843 => Loss: 35.699887\n",
      "Iteration  844 => Loss: 35.662187\n",
      "Iteration  845 => Loss: 35.624687\n",
      "Iteration  846 => Loss: 35.587387\n",
      "Iteration  847 => Loss: 35.550287\n",
      "Iteration  848 => Loss: 35.513387\n",
      "Iteration  849 => Loss: 35.476687\n",
      "Iteration  850 => Loss: 35.440187\n",
      "Iteration  851 => Loss: 35.403887\n",
      "Iteration  852 => Loss: 35.367787\n",
      "Iteration  853 => Loss: 35.331887\n",
      "Iteration  854 => Loss: 35.296187\n",
      "Iteration  855 => Loss: 35.260687\n",
      "Iteration  856 => Loss: 35.225387\n",
      "Iteration  857 => Loss: 35.190287\n",
      "Iteration  858 => Loss: 35.189953\n",
      "Iteration  859 => Loss: 35.152520\n",
      "Iteration  860 => Loss: 35.115287\n",
      "Iteration  861 => Loss: 35.078253\n",
      "Iteration  862 => Loss: 35.041420\n",
      "Iteration  863 => Loss: 35.004787\n",
      "Iteration  864 => Loss: 34.968353\n",
      "Iteration  865 => Loss: 34.932120\n",
      "Iteration  866 => Loss: 34.896087\n",
      "Iteration  867 => Loss: 34.860253\n",
      "Iteration  868 => Loss: 34.824620\n",
      "Iteration  869 => Loss: 34.789187\n",
      "Iteration  870 => Loss: 34.753953\n",
      "Iteration  871 => Loss: 34.718920\n",
      "Iteration  872 => Loss: 34.684087\n",
      "Iteration  873 => Loss: 34.649453\n",
      "Iteration  874 => Loss: 34.615020\n",
      "Iteration  875 => Loss: 34.580787\n",
      "Iteration  876 => Loss: 34.546753\n",
      "Iteration  877 => Loss: 34.544580\n",
      "Iteration  878 => Loss: 34.508213\n",
      "Iteration  879 => Loss: 34.472047\n",
      "Iteration  880 => Loss: 34.436080\n",
      "Iteration  881 => Loss: 34.400313\n",
      "Iteration  882 => Loss: 34.364747\n",
      "Iteration  883 => Loss: 34.329380\n",
      "Iteration  884 => Loss: 34.294213\n",
      "Iteration  885 => Loss: 34.259247\n",
      "Iteration  886 => Loss: 34.224480\n",
      "Iteration  887 => Loss: 34.189913\n",
      "Iteration  888 => Loss: 34.155547\n",
      "Iteration  889 => Loss: 34.121380\n",
      "Iteration  890 => Loss: 34.087413\n",
      "Iteration  891 => Loss: 34.053647\n",
      "Iteration  892 => Loss: 34.020080\n",
      "Iteration  893 => Loss: 33.986713\n",
      "Iteration  894 => Loss: 33.953547\n",
      "Iteration  895 => Loss: 33.952067\n",
      "Iteration  896 => Loss: 33.916567\n",
      "Iteration  897 => Loss: 33.881267\n",
      "Iteration  898 => Loss: 33.846167\n",
      "Iteration  899 => Loss: 33.811267\n",
      "Iteration  900 => Loss: 33.776567\n",
      "Iteration  901 => Loss: 33.742067\n",
      "Iteration  902 => Loss: 33.707767\n",
      "Iteration  903 => Loss: 33.673667\n",
      "Iteration  904 => Loss: 33.639767\n",
      "Iteration  905 => Loss: 33.606067\n",
      "Iteration  906 => Loss: 33.572567\n",
      "Iteration  907 => Loss: 33.539267\n",
      "Iteration  908 => Loss: 33.506167\n",
      "Iteration  909 => Loss: 33.473267\n",
      "Iteration  910 => Loss: 33.440567\n",
      "Iteration  911 => Loss: 33.408067\n",
      "Iteration  912 => Loss: 33.375767\n",
      "Iteration  913 => Loss: 33.374980\n",
      "Iteration  914 => Loss: 33.340347\n",
      "Iteration  915 => Loss: 33.305913\n",
      "Iteration  916 => Loss: 33.271680\n",
      "Iteration  917 => Loss: 33.237647\n",
      "Iteration  918 => Loss: 33.203813\n",
      "Iteration  919 => Loss: 33.170180\n",
      "Iteration  920 => Loss: 33.136747\n",
      "Iteration  921 => Loss: 33.103513\n",
      "Iteration  922 => Loss: 33.070480\n",
      "Iteration  923 => Loss: 33.037647\n",
      "Iteration  924 => Loss: 33.005013\n",
      "Iteration  925 => Loss: 32.972580\n",
      "Iteration  926 => Loss: 32.940347\n",
      "Iteration  927 => Loss: 32.908313\n",
      "Iteration  928 => Loss: 32.876480\n",
      "Iteration  929 => Loss: 32.844847\n",
      "Iteration  930 => Loss: 32.813413\n",
      "Iteration  931 => Loss: 32.813320\n",
      "Iteration  932 => Loss: 32.779553\n",
      "Iteration  933 => Loss: 32.745987\n",
      "Iteration  934 => Loss: 32.712620\n",
      "Iteration  935 => Loss: 32.679453\n",
      "Iteration  936 => Loss: 32.646487\n",
      "Iteration  937 => Loss: 32.613720\n",
      "Iteration  938 => Loss: 32.581153\n",
      "Iteration  939 => Loss: 32.548787\n",
      "Iteration  940 => Loss: 32.516620\n",
      "Iteration  941 => Loss: 32.484653\n",
      "Iteration  942 => Loss: 32.452887\n",
      "Iteration  943 => Loss: 32.421320\n",
      "Iteration  944 => Loss: 32.389953\n",
      "Iteration  945 => Loss: 32.358787\n",
      "Iteration  946 => Loss: 32.327820\n",
      "Iteration  947 => Loss: 32.297053\n",
      "Iteration  948 => Loss: 32.266487\n",
      "Iteration  949 => Loss: 32.236120\n",
      "Iteration  950 => Loss: 32.234187\n",
      "Iteration  951 => Loss: 32.201487\n",
      "Iteration  952 => Loss: 32.168987\n",
      "Iteration  953 => Loss: 32.136687\n",
      "Iteration  954 => Loss: 32.104587\n",
      "Iteration  955 => Loss: 32.072687\n",
      "Iteration  956 => Loss: 32.040987\n",
      "Iteration  957 => Loss: 32.009487\n",
      "Iteration  958 => Loss: 31.978187\n",
      "Iteration  959 => Loss: 31.947087\n",
      "Iteration  960 => Loss: 31.916187\n",
      "Iteration  961 => Loss: 31.885487\n",
      "Iteration  962 => Loss: 31.854987\n",
      "Iteration  963 => Loss: 31.824687\n",
      "Iteration  964 => Loss: 31.794587\n",
      "Iteration  965 => Loss: 31.764687\n",
      "Iteration  966 => Loss: 31.734987\n",
      "Iteration  967 => Loss: 31.705487\n",
      "Iteration  968 => Loss: 31.704247\n",
      "Iteration  969 => Loss: 31.672413\n",
      "Iteration  970 => Loss: 31.640780\n",
      "Iteration  971 => Loss: 31.609347\n",
      "Iteration  972 => Loss: 31.578113\n",
      "Iteration  973 => Loss: 31.547080\n",
      "Iteration  974 => Loss: 31.516247\n",
      "Iteration  975 => Loss: 31.485613\n",
      "Iteration  976 => Loss: 31.455180\n",
      "Iteration  977 => Loss: 31.424947\n",
      "Iteration  978 => Loss: 31.394913\n",
      "Iteration  979 => Loss: 31.365080\n",
      "Iteration  980 => Loss: 31.335447\n",
      "Iteration  981 => Loss: 31.306013\n",
      "Iteration  982 => Loss: 31.276780\n",
      "Iteration  983 => Loss: 31.247747\n",
      "Iteration  984 => Loss: 31.218913\n",
      "Iteration  985 => Loss: 31.190280\n",
      "Iteration  986 => Loss: 31.189733\n",
      "Iteration  987 => Loss: 31.158767\n",
      "Iteration  988 => Loss: 31.128000\n",
      "Iteration  989 => Loss: 31.097433\n",
      "Iteration  990 => Loss: 31.067067\n",
      "Iteration  991 => Loss: 31.036900\n",
      "Iteration  992 => Loss: 31.006933\n",
      "Iteration  993 => Loss: 30.977167\n",
      "Iteration  994 => Loss: 30.947600\n",
      "Iteration  995 => Loss: 30.918233\n",
      "Iteration  996 => Loss: 30.889067\n",
      "Iteration  997 => Loss: 30.860100\n",
      "Iteration  998 => Loss: 30.831333\n",
      "Iteration  999 => Loss: 30.802767\n",
      "Iteration 1000 => Loss: 30.774400\n",
      "Iteration 1001 => Loss: 30.746233\n",
      "Iteration 1002 => Loss: 30.718267\n",
      "Iteration 1003 => Loss: 30.690500\n",
      "Iteration 1004 => Loss: 30.662933\n",
      "Iteration 1005 => Loss: 30.660547\n",
      "Iteration 1006 => Loss: 30.630647\n",
      "Iteration 1007 => Loss: 30.600947\n",
      "Iteration 1008 => Loss: 30.571447\n",
      "Iteration 1009 => Loss: 30.542147\n",
      "Iteration 1010 => Loss: 30.513047\n",
      "Iteration 1011 => Loss: 30.484147\n",
      "Iteration 1012 => Loss: 30.455447\n",
      "Iteration 1013 => Loss: 30.426947\n",
      "Iteration 1014 => Loss: 30.398647\n",
      "Iteration 1015 => Loss: 30.370547\n",
      "Iteration 1016 => Loss: 30.342647\n",
      "Iteration 1017 => Loss: 30.314947\n",
      "Iteration 1018 => Loss: 30.287447\n",
      "Iteration 1019 => Loss: 30.260147\n",
      "Iteration 1020 => Loss: 30.233047\n",
      "Iteration 1021 => Loss: 30.206147\n",
      "Iteration 1022 => Loss: 30.179447\n",
      "Iteration 1023 => Loss: 30.177753\n",
      "Iteration 1024 => Loss: 30.148720\n",
      "Iteration 1025 => Loss: 30.119887\n",
      "Iteration 1026 => Loss: 30.091253\n",
      "Iteration 1027 => Loss: 30.062820\n",
      "Iteration 1028 => Loss: 30.034587\n",
      "Iteration 1029 => Loss: 30.006553\n",
      "Iteration 1030 => Loss: 29.978720\n",
      "Iteration 1031 => Loss: 29.951087\n",
      "Iteration 1032 => Loss: 29.923653\n",
      "Iteration 1033 => Loss: 29.896420\n",
      "Iteration 1034 => Loss: 29.869387\n",
      "Iteration 1035 => Loss: 29.842553\n",
      "Iteration 1036 => Loss: 29.815920\n",
      "Iteration 1037 => Loss: 29.789487\n",
      "Iteration 1038 => Loss: 29.763253\n",
      "Iteration 1039 => Loss: 29.737220\n",
      "Iteration 1040 => Loss: 29.711387\n",
      "Iteration 1041 => Loss: 29.710387\n",
      "Iteration 1042 => Loss: 29.682220\n",
      "Iteration 1043 => Loss: 29.654253\n",
      "Iteration 1044 => Loss: 29.626487\n",
      "Iteration 1045 => Loss: 29.598920\n",
      "Iteration 1046 => Loss: 29.571553\n",
      "Iteration 1047 => Loss: 29.544387\n",
      "Iteration 1048 => Loss: 29.517420\n",
      "Iteration 1049 => Loss: 29.490653\n",
      "Iteration 1050 => Loss: 29.464087\n",
      "Iteration 1051 => Loss: 29.437720\n",
      "Iteration 1052 => Loss: 29.411553\n",
      "Iteration 1053 => Loss: 29.385587\n",
      "Iteration 1054 => Loss: 29.359820\n",
      "Iteration 1055 => Loss: 29.334253\n",
      "Iteration 1056 => Loss: 29.308887\n",
      "Iteration 1057 => Loss: 29.283720\n",
      "Iteration 1058 => Loss: 29.258753\n",
      "Iteration 1059 => Loss: 29.258447\n",
      "Iteration 1060 => Loss: 29.231147\n",
      "Iteration 1061 => Loss: 29.204047\n",
      "Iteration 1062 => Loss: 29.177147\n",
      "Iteration 1063 => Loss: 29.150447\n",
      "Iteration 1064 => Loss: 29.123947\n",
      "Iteration 1065 => Loss: 29.097647\n",
      "Iteration 1066 => Loss: 29.071547\n",
      "Iteration 1067 => Loss: 29.045647\n",
      "Iteration 1068 => Loss: 29.019947\n",
      "Iteration 1069 => Loss: 28.994447\n",
      "Iteration 1070 => Loss: 28.969147\n",
      "Iteration 1071 => Loss: 28.944047\n",
      "Iteration 1072 => Loss: 28.919147\n",
      "Iteration 1073 => Loss: 28.894447\n",
      "Iteration 1074 => Loss: 28.869947\n",
      "Iteration 1075 => Loss: 28.845647\n",
      "Iteration 1076 => Loss: 28.821547\n",
      "Iteration 1077 => Loss: 28.797647\n",
      "Iteration 1078 => Loss: 28.795500\n",
      "Iteration 1079 => Loss: 28.769267\n",
      "Iteration 1080 => Loss: 28.743233\n",
      "Iteration 1081 => Loss: 28.717400\n",
      "Iteration 1082 => Loss: 28.691767\n",
      "Iteration 1083 => Loss: 28.666333\n",
      "Iteration 1084 => Loss: 28.641100\n",
      "Iteration 1085 => Loss: 28.616067\n",
      "Iteration 1086 => Loss: 28.591233\n",
      "Iteration 1087 => Loss: 28.566600\n",
      "Iteration 1088 => Loss: 28.542167\n",
      "Iteration 1089 => Loss: 28.517933\n",
      "Iteration 1090 => Loss: 28.493900\n",
      "Iteration 1091 => Loss: 28.470067\n",
      "Iteration 1092 => Loss: 28.446433\n",
      "Iteration 1093 => Loss: 28.423000\n",
      "Iteration 1094 => Loss: 28.399767\n",
      "Iteration 1095 => Loss: 28.376733\n",
      "Iteration 1096 => Loss: 28.375280\n",
      "Iteration 1097 => Loss: 28.349913\n",
      "Iteration 1098 => Loss: 28.324747\n",
      "Iteration 1099 => Loss: 28.299780\n",
      "Iteration 1100 => Loss: 28.275013\n",
      "Iteration 1101 => Loss: 28.250447\n",
      "Iteration 1102 => Loss: 28.226080\n",
      "Iteration 1103 => Loss: 28.201913\n",
      "Iteration 1104 => Loss: 28.177947\n",
      "Iteration 1105 => Loss: 28.154180\n",
      "Iteration 1106 => Loss: 28.130613\n",
      "Iteration 1107 => Loss: 28.107247\n",
      "Iteration 1108 => Loss: 28.084080\n",
      "Iteration 1109 => Loss: 28.061113\n",
      "Iteration 1110 => Loss: 28.038347\n",
      "Iteration 1111 => Loss: 28.015780\n",
      "Iteration 1112 => Loss: 27.993413\n",
      "Iteration 1113 => Loss: 27.971247\n",
      "Iteration 1114 => Loss: 27.970487\n",
      "Iteration 1115 => Loss: 27.945987\n",
      "Iteration 1116 => Loss: 27.921687\n",
      "Iteration 1117 => Loss: 27.897587\n",
      "Iteration 1118 => Loss: 27.873687\n",
      "Iteration 1119 => Loss: 27.849987\n",
      "Iteration 1120 => Loss: 27.826487\n",
      "Iteration 1121 => Loss: 27.803187\n",
      "Iteration 1122 => Loss: 27.780087\n",
      "Iteration 1123 => Loss: 27.757187\n",
      "Iteration 1124 => Loss: 27.734487\n",
      "Iteration 1125 => Loss: 27.711987\n",
      "Iteration 1126 => Loss: 27.689687\n",
      "Iteration 1127 => Loss: 27.667587\n",
      "Iteration 1128 => Loss: 27.645687\n",
      "Iteration 1129 => Loss: 27.623987\n",
      "Iteration 1130 => Loss: 27.602487\n",
      "Iteration 1131 => Loss: 27.581187\n",
      "Iteration 1132 => Loss: 27.581120\n",
      "Iteration 1133 => Loss: 27.557487\n",
      "Iteration 1134 => Loss: 27.534053\n",
      "Iteration 1135 => Loss: 27.510820\n",
      "Iteration 1136 => Loss: 27.487787\n",
      "Iteration 1137 => Loss: 27.464953\n",
      "Iteration 1138 => Loss: 27.442320\n",
      "Iteration 1139 => Loss: 27.419887\n",
      "Iteration 1140 => Loss: 27.397653\n",
      "Iteration 1141 => Loss: 27.375620\n",
      "Iteration 1142 => Loss: 27.353787\n",
      "Iteration 1143 => Loss: 27.332153\n",
      "Iteration 1144 => Loss: 27.310720\n",
      "Iteration 1145 => Loss: 27.289487\n",
      "Iteration 1146 => Loss: 27.268453\n",
      "Iteration 1147 => Loss: 27.247620\n",
      "Iteration 1148 => Loss: 27.226987\n",
      "Iteration 1149 => Loss: 27.206553\n",
      "Iteration 1150 => Loss: 27.186320\n",
      "Iteration 1151 => Loss: 27.184413\n",
      "Iteration 1152 => Loss: 27.161847\n",
      "Iteration 1153 => Loss: 27.139480\n",
      "Iteration 1154 => Loss: 27.117313\n",
      "Iteration 1155 => Loss: 27.095347\n",
      "Iteration 1156 => Loss: 27.073580\n",
      "Iteration 1157 => Loss: 27.052013\n",
      "Iteration 1158 => Loss: 27.030647\n",
      "Iteration 1159 => Loss: 27.009480\n",
      "Iteration 1160 => Loss: 26.988513\n",
      "Iteration 1161 => Loss: 26.967747\n",
      "Iteration 1162 => Loss: 26.947180\n",
      "Iteration 1163 => Loss: 26.926813\n",
      "Iteration 1164 => Loss: 26.906647\n",
      "Iteration 1165 => Loss: 26.886680\n",
      "Iteration 1166 => Loss: 26.866913\n",
      "Iteration 1167 => Loss: 26.847347\n",
      "Iteration 1168 => Loss: 26.827980\n",
      "Iteration 1169 => Loss: 26.826767\n",
      "Iteration 1170 => Loss: 26.805067\n",
      "Iteration 1171 => Loss: 26.783567\n",
      "Iteration 1172 => Loss: 26.762267\n",
      "Iteration 1173 => Loss: 26.741167\n",
      "Iteration 1174 => Loss: 26.720267\n",
      "Iteration 1175 => Loss: 26.699567\n",
      "Iteration 1176 => Loss: 26.679067\n",
      "Iteration 1177 => Loss: 26.658767\n",
      "Iteration 1178 => Loss: 26.638667\n",
      "Iteration 1179 => Loss: 26.618767\n",
      "Iteration 1180 => Loss: 26.599067\n",
      "Iteration 1181 => Loss: 26.579567\n",
      "Iteration 1182 => Loss: 26.560267\n",
      "Iteration 1183 => Loss: 26.541167\n",
      "Iteration 1184 => Loss: 26.522267\n",
      "Iteration 1185 => Loss: 26.503567\n",
      "Iteration 1186 => Loss: 26.485067\n",
      "Iteration 1187 => Loss: 26.484547\n",
      "Iteration 1188 => Loss: 26.463713\n",
      "Iteration 1189 => Loss: 26.443080\n",
      "Iteration 1190 => Loss: 26.422647\n",
      "Iteration 1191 => Loss: 26.402413\n",
      "Iteration 1192 => Loss: 26.382380\n",
      "Iteration 1193 => Loss: 26.362547\n",
      "Iteration 1194 => Loss: 26.342913\n",
      "Iteration 1195 => Loss: 26.323480\n",
      "Iteration 1196 => Loss: 26.304247\n",
      "Iteration 1197 => Loss: 26.285213\n",
      "Iteration 1198 => Loss: 26.266380\n",
      "Iteration 1199 => Loss: 26.247747\n",
      "Iteration 1200 => Loss: 26.229313\n",
      "Iteration 1201 => Loss: 26.211080\n",
      "Iteration 1202 => Loss: 26.193047\n",
      "Iteration 1203 => Loss: 26.175213\n",
      "Iteration 1204 => Loss: 26.157580\n",
      "Iteration 1205 => Loss: 26.140147\n",
      "Iteration 1206 => Loss: 26.137787\n",
      "Iteration 1207 => Loss: 26.118020\n",
      "Iteration 1208 => Loss: 26.098453\n",
      "Iteration 1209 => Loss: 26.079087\n",
      "Iteration 1210 => Loss: 26.059920\n",
      "Iteration 1211 => Loss: 26.040953\n",
      "Iteration 1212 => Loss: 26.022187\n",
      "Iteration 1213 => Loss: 26.003620\n",
      "Iteration 1214 => Loss: 25.985253\n",
      "Iteration 1215 => Loss: 25.967087\n",
      "Iteration 1216 => Loss: 25.949120\n",
      "Iteration 1217 => Loss: 25.931353\n",
      "Iteration 1218 => Loss: 25.913787\n",
      "Iteration 1219 => Loss: 25.896420\n",
      "Iteration 1220 => Loss: 25.879253\n",
      "Iteration 1221 => Loss: 25.862287\n",
      "Iteration 1222 => Loss: 25.845520\n",
      "Iteration 1223 => Loss: 25.828953\n",
      "Iteration 1224 => Loss: 25.827287\n",
      "Iteration 1225 => Loss: 25.808387\n",
      "Iteration 1226 => Loss: 25.789687\n",
      "Iteration 1227 => Loss: 25.771187\n",
      "Iteration 1228 => Loss: 25.752887\n",
      "Iteration 1229 => Loss: 25.734787\n",
      "Iteration 1230 => Loss: 25.716887\n",
      "Iteration 1231 => Loss: 25.699187\n",
      "Iteration 1232 => Loss: 25.681687\n",
      "Iteration 1233 => Loss: 25.664387\n",
      "Iteration 1234 => Loss: 25.647287\n",
      "Iteration 1235 => Loss: 25.630387\n",
      "Iteration 1236 => Loss: 25.613687\n",
      "Iteration 1237 => Loss: 25.597187\n",
      "Iteration 1238 => Loss: 25.580887\n",
      "Iteration 1239 => Loss: 25.564787\n",
      "Iteration 1240 => Loss: 25.548887\n",
      "Iteration 1241 => Loss: 25.533187\n",
      "Iteration 1242 => Loss: 25.532213\n",
      "Iteration 1243 => Loss: 25.514180\n",
      "Iteration 1244 => Loss: 25.496347\n",
      "Iteration 1245 => Loss: 25.478713\n",
      "Iteration 1246 => Loss: 25.461280\n",
      "Iteration 1247 => Loss: 25.444047\n",
      "Iteration 1248 => Loss: 25.427013\n",
      "Iteration 1249 => Loss: 25.410180\n",
      "Iteration 1250 => Loss: 25.393547\n",
      "Iteration 1251 => Loss: 25.377113\n",
      "Iteration 1252 => Loss: 25.360880\n",
      "Iteration 1253 => Loss: 25.344847\n",
      "Iteration 1254 => Loss: 25.329013\n",
      "Iteration 1255 => Loss: 25.313380\n",
      "Iteration 1256 => Loss: 25.297947\n",
      "Iteration 1257 => Loss: 25.282713\n",
      "Iteration 1258 => Loss: 25.267680\n",
      "Iteration 1259 => Loss: 25.252847\n",
      "Iteration 1260 => Loss: 25.252567\n",
      "Iteration 1261 => Loss: 25.235400\n",
      "Iteration 1262 => Loss: 25.218433\n",
      "Iteration 1263 => Loss: 25.201667\n",
      "Iteration 1264 => Loss: 25.185100\n",
      "Iteration 1265 => Loss: 25.168733\n",
      "Iteration 1266 => Loss: 25.152567\n",
      "Iteration 1267 => Loss: 25.136600\n",
      "Iteration 1268 => Loss: 25.120833\n",
      "Iteration 1269 => Loss: 25.105267\n",
      "Iteration 1270 => Loss: 25.089900\n",
      "Iteration 1271 => Loss: 25.074733\n",
      "Iteration 1272 => Loss: 25.059767\n",
      "Iteration 1273 => Loss: 25.045000\n",
      "Iteration 1274 => Loss: 25.030433\n",
      "Iteration 1275 => Loss: 25.016067\n",
      "Iteration 1276 => Loss: 25.001900\n",
      "Iteration 1277 => Loss: 24.987933\n",
      "Iteration 1278 => Loss: 24.974167\n",
      "Iteration 1279 => Loss: 24.972047\n",
      "Iteration 1280 => Loss: 24.955947\n",
      "Iteration 1281 => Loss: 24.940047\n",
      "Iteration 1282 => Loss: 24.924347\n",
      "Iteration 1283 => Loss: 24.908847\n",
      "Iteration 1284 => Loss: 24.893547\n",
      "Iteration 1285 => Loss: 24.878447\n",
      "Iteration 1286 => Loss: 24.863547\n",
      "Iteration 1287 => Loss: 24.848847\n",
      "Iteration 1288 => Loss: 24.834347\n",
      "Iteration 1289 => Loss: 24.820047\n",
      "Iteration 1290 => Loss: 24.805947\n",
      "Iteration 1291 => Loss: 24.792047\n",
      "Iteration 1292 => Loss: 24.778347\n",
      "Iteration 1293 => Loss: 24.764847\n",
      "Iteration 1294 => Loss: 24.751547\n",
      "Iteration 1295 => Loss: 24.738447\n",
      "Iteration 1296 => Loss: 24.725547\n",
      "Iteration 1297 => Loss: 24.724120\n",
      "Iteration 1298 => Loss: 24.708887\n",
      "Iteration 1299 => Loss: 24.693853\n",
      "Iteration 1300 => Loss: 24.679020\n",
      "Iteration 1301 => Loss: 24.664387\n",
      "Iteration 1302 => Loss: 24.649953\n",
      "Iteration 1303 => Loss: 24.635720\n",
      "Iteration 1304 => Loss: 24.621687\n",
      "Iteration 1305 => Loss: 24.607853\n",
      "Iteration 1306 => Loss: 24.594220\n",
      "Iteration 1307 => Loss: 24.580787\n",
      "Iteration 1308 => Loss: 24.567553\n",
      "Iteration 1309 => Loss: 24.554520\n",
      "Iteration 1310 => Loss: 24.541687\n",
      "Iteration 1311 => Loss: 24.529053\n",
      "Iteration 1312 => Loss: 24.516620\n",
      "Iteration 1313 => Loss: 24.504387\n",
      "Iteration 1314 => Loss: 24.492353\n",
      "Iteration 1315 => Loss: 24.491620\n",
      "Iteration 1316 => Loss: 24.477253\n",
      "Iteration 1317 => Loss: 24.463087\n",
      "Iteration 1318 => Loss: 24.449120\n",
      "Iteration 1319 => Loss: 24.435353\n",
      "Iteration 1320 => Loss: 24.421787\n",
      "Iteration 1321 => Loss: 24.408420\n",
      "Iteration 1322 => Loss: 24.395253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1323 => Loss: 24.382287\n",
      "Iteration 1324 => Loss: 24.369520\n",
      "Iteration 1325 => Loss: 24.356953\n",
      "Iteration 1326 => Loss: 24.344587\n",
      "Iteration 1327 => Loss: 24.332420\n",
      "Iteration 1328 => Loss: 24.320453\n",
      "Iteration 1329 => Loss: 24.308687\n",
      "Iteration 1330 => Loss: 24.297120\n",
      "Iteration 1331 => Loss: 24.285753\n",
      "Iteration 1332 => Loss: 24.274587\n",
      "Iteration 1333 => Loss: 24.274547\n",
      "Iteration 1334 => Loss: 24.261047\n",
      "Iteration 1335 => Loss: 24.247747\n",
      "Iteration 1336 => Loss: 24.234647\n",
      "Iteration 1337 => Loss: 24.221747\n",
      "Iteration 1338 => Loss: 24.209047\n",
      "Iteration 1339 => Loss: 24.196547\n",
      "Iteration 1340 => Loss: 24.184247\n",
      "Iteration 1341 => Loss: 24.172147\n",
      "Iteration 1342 => Loss: 24.160247\n",
      "Iteration 1343 => Loss: 24.148547\n",
      "Iteration 1344 => Loss: 24.137047\n",
      "Iteration 1345 => Loss: 24.125747\n",
      "Iteration 1346 => Loss: 24.114647\n",
      "Iteration 1347 => Loss: 24.103747\n",
      "Iteration 1348 => Loss: 24.093047\n",
      "Iteration 1349 => Loss: 24.082547\n",
      "Iteration 1350 => Loss: 24.072247\n",
      "Iteration 1351 => Loss: 24.062147\n",
      "Iteration 1352 => Loss: 24.060267\n",
      "Iteration 1353 => Loss: 24.047833\n",
      "Iteration 1354 => Loss: 24.035600\n",
      "Iteration 1355 => Loss: 24.023567\n",
      "Iteration 1356 => Loss: 24.011733\n",
      "Iteration 1357 => Loss: 24.000100\n",
      "Iteration 1358 => Loss: 23.988667\n",
      "Iteration 1359 => Loss: 23.977433\n",
      "Iteration 1360 => Loss: 23.966400\n",
      "Iteration 1361 => Loss: 23.955567\n",
      "Iteration 1362 => Loss: 23.944933\n",
      "Iteration 1363 => Loss: 23.934500\n",
      "Iteration 1364 => Loss: 23.924267\n",
      "Iteration 1365 => Loss: 23.914233\n",
      "Iteration 1366 => Loss: 23.904400\n",
      "Iteration 1367 => Loss: 23.894767\n",
      "Iteration 1368 => Loss: 23.885333\n",
      "Iteration 1369 => Loss: 23.876100\n",
      "Iteration 1370 => Loss: 23.874913\n",
      "Iteration 1371 => Loss: 23.863347\n",
      "Iteration 1372 => Loss: 23.851980\n",
      "Iteration 1373 => Loss: 23.840813\n",
      "Iteration 1374 => Loss: 23.829847\n",
      "Iteration 1375 => Loss: 23.819080\n",
      "Iteration 1376 => Loss: 23.808513\n",
      "Iteration 1377 => Loss: 23.798147\n",
      "Iteration 1378 => Loss: 23.787980\n",
      "Iteration 1379 => Loss: 23.778013\n",
      "Iteration 1380 => Loss: 23.768247\n",
      "Iteration 1381 => Loss: 23.758680\n",
      "Iteration 1382 => Loss: 23.749313\n",
      "Iteration 1383 => Loss: 23.740147\n",
      "Iteration 1384 => Loss: 23.731180\n",
      "Iteration 1385 => Loss: 23.722413\n",
      "Iteration 1386 => Loss: 23.713847\n",
      "Iteration 1387 => Loss: 23.705480\n",
      "Iteration 1388 => Loss: 23.704987\n",
      "Iteration 1389 => Loss: 23.694287\n",
      "Iteration 1390 => Loss: 23.683787\n",
      "Iteration 1391 => Loss: 23.673487\n",
      "Iteration 1392 => Loss: 23.663387\n",
      "Iteration 1393 => Loss: 23.653487\n",
      "Iteration 1394 => Loss: 23.643787\n",
      "Iteration 1395 => Loss: 23.634287\n",
      "Iteration 1396 => Loss: 23.624987\n",
      "Iteration 1397 => Loss: 23.615887\n",
      "Iteration 1398 => Loss: 23.606987\n",
      "Iteration 1399 => Loss: 23.598287\n",
      "Iteration 1400 => Loss: 23.589787\n",
      "Iteration 1401 => Loss: 23.581487\n",
      "Iteration 1402 => Loss: 23.573387\n",
      "Iteration 1403 => Loss: 23.565487\n",
      "Iteration 1404 => Loss: 23.557787\n",
      "Iteration 1405 => Loss: 23.550287\n",
      "Iteration 1406 => Loss: 23.542987\n",
      "Iteration 1407 => Loss: 23.540653\n",
      "Iteration 1408 => Loss: 23.531020\n",
      "Iteration 1409 => Loss: 23.521587\n",
      "Iteration 1410 => Loss: 23.512353\n",
      "Iteration 1411 => Loss: 23.503320\n",
      "Iteration 1412 => Loss: 23.494487\n",
      "Iteration 1413 => Loss: 23.485853\n",
      "Iteration 1414 => Loss: 23.477420\n",
      "Iteration 1415 => Loss: 23.469187\n",
      "Iteration 1416 => Loss: 23.461153\n",
      "Iteration 1417 => Loss: 23.453320\n",
      "Iteration 1418 => Loss: 23.445687\n",
      "Iteration 1419 => Loss: 23.438253\n",
      "Iteration 1420 => Loss: 23.431020\n",
      "Iteration 1421 => Loss: 23.423987\n",
      "Iteration 1422 => Loss: 23.417153\n",
      "Iteration 1423 => Loss: 23.410520\n",
      "Iteration 1424 => Loss: 23.404087\n",
      "Iteration 1425 => Loss: 23.402447\n",
      "Iteration 1426 => Loss: 23.393680\n",
      "Iteration 1427 => Loss: 23.385113\n",
      "Iteration 1428 => Loss: 23.376747\n",
      "Iteration 1429 => Loss: 23.368580\n",
      "Iteration 1430 => Loss: 23.360613\n",
      "Iteration 1431 => Loss: 23.352847\n",
      "Iteration 1432 => Loss: 23.345280\n",
      "Iteration 1433 => Loss: 23.337913\n",
      "Iteration 1434 => Loss: 23.330747\n",
      "Iteration 1435 => Loss: 23.323780\n",
      "Iteration 1436 => Loss: 23.317013\n",
      "Iteration 1437 => Loss: 23.310447\n",
      "Iteration 1438 => Loss: 23.304080\n",
      "Iteration 1439 => Loss: 23.297913\n",
      "Iteration 1440 => Loss: 23.291947\n",
      "Iteration 1441 => Loss: 23.286180\n",
      "Iteration 1442 => Loss: 23.280613\n",
      "Iteration 1443 => Loss: 23.279667\n",
      "Iteration 1444 => Loss: 23.271767\n",
      "Iteration 1445 => Loss: 23.264067\n",
      "Iteration 1446 => Loss: 23.256567\n",
      "Iteration 1447 => Loss: 23.249267\n",
      "Iteration 1448 => Loss: 23.242167\n",
      "Iteration 1449 => Loss: 23.235267\n",
      "Iteration 1450 => Loss: 23.228567\n",
      "Iteration 1451 => Loss: 23.222067\n",
      "Iteration 1452 => Loss: 23.215767\n",
      "Iteration 1453 => Loss: 23.209667\n",
      "Iteration 1454 => Loss: 23.203767\n",
      "Iteration 1455 => Loss: 23.198067\n",
      "Iteration 1456 => Loss: 23.192567\n",
      "Iteration 1457 => Loss: 23.187267\n",
      "Iteration 1458 => Loss: 23.182167\n",
      "Iteration 1459 => Loss: 23.177267\n",
      "Iteration 1460 => Loss: 23.172567\n",
      "Iteration 1461 => Loss: 23.172313\n",
      "Iteration 1462 => Loss: 23.165280\n",
      "Iteration 1463 => Loss: 23.158447\n",
      "Iteration 1464 => Loss: 23.151813\n",
      "Iteration 1465 => Loss: 23.145380\n",
      "Iteration 1466 => Loss: 23.139147\n",
      "Iteration 1467 => Loss: 23.133113\n",
      "Iteration 1468 => Loss: 23.127280\n",
      "Iteration 1469 => Loss: 23.121647\n",
      "Iteration 1470 => Loss: 23.116213\n",
      "Iteration 1471 => Loss: 23.110980\n",
      "Iteration 1472 => Loss: 23.105947\n",
      "Iteration 1473 => Loss: 23.101113\n",
      "Iteration 1474 => Loss: 23.096480\n",
      "Iteration 1475 => Loss: 23.092047\n",
      "Iteration 1476 => Loss: 23.087813\n",
      "Iteration 1477 => Loss: 23.083780\n",
      "Iteration 1478 => Loss: 23.079947\n",
      "Iteration 1479 => Loss: 23.076313\n",
      "Iteration 1480 => Loss: 23.074220\n",
      "Iteration 1481 => Loss: 23.068253\n",
      "Iteration 1482 => Loss: 23.062487\n",
      "Iteration 1483 => Loss: 23.056920\n",
      "Iteration 1484 => Loss: 23.051553\n",
      "Iteration 1485 => Loss: 23.046387\n",
      "Iteration 1486 => Loss: 23.041420\n",
      "Iteration 1487 => Loss: 23.036653\n",
      "Iteration 1488 => Loss: 23.032087\n",
      "Iteration 1489 => Loss: 23.027720\n",
      "Iteration 1490 => Loss: 23.023553\n",
      "Iteration 1491 => Loss: 23.019587\n",
      "Iteration 1492 => Loss: 23.015820\n",
      "Iteration 1493 => Loss: 23.012253\n",
      "Iteration 1494 => Loss: 23.008887\n",
      "Iteration 1495 => Loss: 23.005720\n",
      "Iteration 1496 => Loss: 23.002753\n",
      "Iteration 1497 => Loss: 22.999987\n",
      "Iteration 1498 => Loss: 22.998587\n",
      "Iteration 1499 => Loss: 22.993487\n",
      "Iteration 1500 => Loss: 22.988587\n",
      "Iteration 1501 => Loss: 22.983887\n",
      "Iteration 1502 => Loss: 22.979387\n",
      "Iteration 1503 => Loss: 22.975087\n",
      "Iteration 1504 => Loss: 22.970987\n",
      "Iteration 1505 => Loss: 22.967087\n",
      "Iteration 1506 => Loss: 22.963387\n",
      "Iteration 1507 => Loss: 22.959887\n",
      "Iteration 1508 => Loss: 22.956587\n",
      "Iteration 1509 => Loss: 22.953487\n",
      "Iteration 1510 => Loss: 22.950587\n",
      "Iteration 1511 => Loss: 22.947887\n",
      "Iteration 1512 => Loss: 22.945387\n",
      "Iteration 1513 => Loss: 22.943087\n",
      "Iteration 1514 => Loss: 22.940987\n",
      "Iteration 1515 => Loss: 22.939087\n",
      "Iteration 1516 => Loss: 22.938380\n",
      "Iteration 1517 => Loss: 22.934147\n",
      "Iteration 1518 => Loss: 22.930113\n",
      "Iteration 1519 => Loss: 22.926280\n",
      "Iteration 1520 => Loss: 22.922647\n",
      "Iteration 1521 => Loss: 22.919213\n",
      "Iteration 1522 => Loss: 22.915980\n",
      "Iteration 1523 => Loss: 22.912947\n",
      "Iteration 1524 => Loss: 22.910113\n",
      "Iteration 1525 => Loss: 22.907480\n",
      "Iteration 1526 => Loss: 22.905047\n",
      "Iteration 1527 => Loss: 22.902813\n",
      "Iteration 1528 => Loss: 22.900780\n",
      "Iteration 1529 => Loss: 22.898947\n",
      "Iteration 1530 => Loss: 22.897313\n",
      "Iteration 1531 => Loss: 22.895880\n",
      "Iteration 1532 => Loss: 22.894647\n",
      "Iteration 1533 => Loss: 22.893613\n",
      "Iteration 1534 => Loss: 22.893600\n",
      "Iteration 1535 => Loss: 22.890233\n",
      "Iteration 1536 => Loss: 22.887067\n",
      "Iteration 1537 => Loss: 22.884100\n",
      "Iteration 1538 => Loss: 22.881333\n",
      "Iteration 1539 => Loss: 22.878767\n",
      "Iteration 1540 => Loss: 22.876400\n",
      "Iteration 1541 => Loss: 22.874233\n",
      "Iteration 1542 => Loss: 22.872267\n",
      "Iteration 1543 => Loss: 22.870500\n",
      "Iteration 1544 => Loss: 22.868933\n",
      "Iteration 1545 => Loss: 22.867567\n",
      "Iteration 1546 => Loss: 22.866400\n",
      "Iteration 1547 => Loss: 22.865433\n",
      "Iteration 1548 => Loss: 22.864667\n",
      "Iteration 1549 => Loss: 22.864100\n",
      "Iteration 1550 => Loss: 22.863733\n",
      "Iteration 1551 => Loss: 22.863567\n"
     ]
    }
   ],
   "source": [
    "w, b = train(X, Y, 10000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1000000000000008 12.929999999999769\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the examples like we did before:\n",
    "plt.xlabel(\"Reservations\", fontsize=20)\n",
    "plt.ylabel(\"Pizzas\", fontsize=20)\n",
    "plt.axis([0, 50, 0, 50])\n",
    "plt.plot(X, Y, \"bo\")\n",
    "\n",
    "# Plot the line:\n",
    "plt.plot([0, 50], [b, predict(50, w, b)], color=\"g\")\n",
    "\n",
    "# Visualize the diagram:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.1299999999998"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservation = 42\n",
    "predict(42, w, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
